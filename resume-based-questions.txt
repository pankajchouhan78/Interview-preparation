Resume Interview Question List - Based on Pankaj Chouhan's Resume

Categories (question ranges):
- General profile and career: Q1–Q20
- Education and transition to software: Q21–Q30
- Current role at Bridgefix: Q31–Q40
- Celery, Redis, Docker, CI/CD and collaboration: Q41–Q50
- Thirdi-AI – marketing analytics project: Q51–Q70
- DHL Logistics Platform – logistics project: Q71–Q90
- ProcureMate – FastAPI and AWS project: Q91–Q110
- Python language and backend skills: Q111–Q130
- Django and Django REST Framework: Q131–Q150
- FastAPI: Q151–Q160
- Celery and Redis: Q161–Q170
- Databases (PostgreSQL and MySQL): Q171–Q180
- Docker, AWS and CI/CD: Q181–Q190
- Soft skills and behavioural questions: Q191–Q200


Q1. Can you walk me through your resume and highlight the most important points?
Answer:
I am a Python backend developer with about one and a half years of experience. Right now I work as an Associate Software Engineer at Bridgefix Technologies in Indore. My main skills are Python, Django, Django REST Framework, FastAPI, Celery, Redis, PostgreSQL, MySQL, Docker and AWS. I have worked on three major projects: Thirdi-AI for marketing analytics, DHL Logistics Platform for shipment tracking, and ProcureMate for tender management. In these projects I built REST APIs, background tasks and database schemas, and helped with deployments. Before that, I completed my B.Sc. in Electronics and Communication with a CGPA of 7.7. Overall, I focus on building reliable, clean and scalable backend services that solve real business problems.

Q2. How would you introduce yourself to an interviewer in one minute based on this resume?
Answer:
My name is Pankaj Chouhan, and I am a Python backend developer with around one and a half years of experience. I currently work as an Associate Software Engineer at Bridgefix Technologies in Indore. I mainly work with Python, Django, Django REST Framework, FastAPI, Celery, Redis, PostgreSQL, MySQL, Docker and AWS. I have built and maintained RESTful APIs, background jobs and database schemas for projects in marketing analytics, logistics and tender management. I enjoy backend development because I like solving logical problems and making systems fast and reliable. In my next role, I want to keep improving my skills in backend architecture, performance and cloud deployments, and contribute to a strong engineering team.

Q3. Why did you choose to become a Python / Backend Developer?
Answer:
I chose Python and backend development because I enjoy working with logic and data more than with visuals. Python is easy to read and write, so it helped me learn programming concepts faster. When I started using Django, DRF and FastAPI, I discovered that I really like designing APIs, models and database schemas. Backend development lets me focus on how data flows through the system, how to keep it consistent, and how to make things perform well under load. I also like that backend work has a big impact on reliability and performance for end users, even if they never see the code directly.

Q4. You mention 1.5+ years of experience; how has your role evolved over this time?
Answer:
At the beginning of my career, my main focus was on learning the codebase, fixing smaller bugs and implementing simple features under guidance. I spent a lot of time understanding how Django and DRF projects are structured, how endpoints are connected to models and serializers, and how deployments work. Over time, I started taking full responsibility for features. Now I can design an API from scratch, plan the database changes, implement the logic, write tests and support deployment. I also participate more in technical discussions, suggest improvements in performance or structure, and help teammates when they face issues. This growth has made me more confident and independent.

Q5. What types of backend systems have you worked on so far?
Answer:
So far I have worked on three main types of backend systems. The first type is marketing analytics, like the Thirdi-AI project, where we integrate with external marketing APIs to collect and aggregate campaign data. The second type is logistics, like the DHL Logistics Platform, where we manage shipments, routes, tracking information and alerts for different user roles. The third type is procurement and tender management, like ProcureMate, where we handle tenders, bids, vendors and large document uploads using FastAPI and AWS. In each of these systems I worked on REST APIs, background processing with Celery, relational databases and sometimes deployment and cloud configuration.

Q6. Among Django, DRF, Flask, and FastAPI, which do you feel strongest in and why?
Answer:
I feel strongest in Django and Django REST Framework, because I have used them the most in real projects at Bridgefix. I am comfortable defining models, serializers, viewsets and routers, and I know how to handle authentication, permissions, filtering, pagination and validation. I have built many CRUD and complex APIs with DRF and connected them to PostgreSQL or MySQL databases. I also have good experience with FastAPI from the ProcureMate project, where I designed endpoints and used Pydantic models. Flask I have used less in production. Overall, I am most productive and confident with Django plus DRF, but I can also work well with FastAPI when needed.

Q7. What kinds of business domains have you worked in (marketing, logistics, tender management, etc.)?
Answer:
I have worked mainly in three business domains. In marketing analytics, through the Thirdi-AI project, I integrated APIs from platforms like Facebook, Google, TikTok and LinkedIn to collect ad performance data. In logistics, through the DHL Logistics Platform, I helped build services for shipment creation, tracking, route management and alerts. In procurement and tender management, through ProcureMate, I worked on features for vendor onboarding, tender listing, bid submission and handling large document uploads. Each domain had different business rules and edge cases, so I learned how to adapt my backend skills to different industries while keeping the core principles of clean APIs and solid data modeling.

Q8. What do you enjoy most about backend development?
Answer:
What I enjoy most about backend development is solving logical problems and designing systems that work reliably behind the scenes. I like thinking about data flow, how to structure database tables, and how different services communicate through APIs. It is satisfying to see that when the backend is well designed, the frontend and other consumers can work smoothly. I also enjoy performance work, like optimizing queries or moving heavy tasks into Celery jobs. Another thing I like is that backend code often has a long life, so good decisions here keep helping the team for a long time.

Q9. What do you find most challenging about backend development?
Answer:
The most challenging part of backend development for me is dealing with complex business logic and many edge cases while keeping the code simple and clean. Often there are many rules, roles and conditions that must all work together correctly. It is easy to miss rare scenarios if requirements are not clear. Another challenge is maintaining good performance as the data and traffic grow. You have to think about indexes, caching, background processing and monitoring. Also, backend changes can affect many parts of the system, so careful testing and communication with the team are very important to avoid breaking other features.

Q10. How do you make sure the code you write is clean and maintainable?
Answer:
To keep my code clean and maintainable, I try to follow a few habits. I keep functions and classes focused on a single responsibility, and if a function becomes too long, I split it into smaller helpers. I choose clear and descriptive names for variables, functions and endpoints so others can quickly understand the purpose. I avoid duplicating logic by extracting common code into reusable utilities or services. I also write basic tests for important business logic and critical APIs. Finally, I take code review seriously, accept feedback from teammates and follow common best practices in Django, DRF and FastAPI.

Q11. Your profile mentions focus on performance—can you give an example where you improved performance?
Answer:
In one project, a reporting API was very slow because it was making multiple database queries inside a loop and doing a lot of work in the request thread. The endpoint sometimes took 5–6 seconds to respond. I analyzed the code and combined several queries using Django ORM with select_related and prefetch_related to reduce round trips. I also moved some heavy calculations into a Celery task that prepared summary data in advance. After these changes, the response time dropped to under one second for most requests. This made the dashboard feel much faster for users and reduced load on the database.

Q12. How do you approach understanding a new codebase when you join a project?
Answer:
When I join a new project, I first try to understand the big picture. I read the README, architecture docs if they exist, and check the main apps or modules. Then I run the project locally and use a few main features to see how it behaves. After that, I open the URL routing and main views or endpoints to see how requests flow through the system. I also look at models to understand the data structure. For specific tasks, I trace the code starting from the endpoint related to that feature. I ask questions when something is not clear.

Q13. What is your usual workflow when you start a new backend feature from a requirement?
Answer:
When I get a new requirement, I first make sure I clearly understand the use case, inputs, outputs and any business rules. I often restate the requirement in my own words to the product owner or senior developer. Then I design the API contract, including URL, method, request body and response format, and think about necessary database changes. Next, I implement models or migrations if needed, then serializers or schemas, then views or route handlers. I write tests or at least manual test cases, and use Postman to verify the endpoints. Finally, I do a self-review, run tests and create a pull request.

Q14. How do you collaborate with frontend developers when designing APIs?
Answer:
When collaborating with frontend developers, I try to communicate early. We discuss what data they need, how many calls are acceptable, and what the response format should be. Sometimes we design a sample JSON response together so they can start working with mock data. I also think about pagination and filtering so that the frontend can load data efficiently. If there is any change in the API, I inform them clearly and update the documentation or shared Postman collection. During development, I am available on chat or calls to quickly clarify doubts. This helps us avoid misunderstandings and rework.

Q15. How do you use tools like Postman in your daily work?
Answer:
I use Postman mainly for testing and debugging APIs. When I build a new endpoint, I first add it to a Postman collection with the correct method, URL, headers and body. Then I send test requests to check that the response status, body and error handling are correct. I also save example requests for different scenarios, such as valid, invalid or missing data. For authenticated APIs, I configure tokens or sessions in Postman so I can quickly test protected endpoints. Postman is also useful to share API examples with frontend developers or QA, so everyone can see how the endpoint should behave.

Q16. Which project from your resume are you most proud of and why?
Answer:
I am most proud of the ProcureMate project. It was one of my first serious experiences with FastAPI and AWS, and I had a big role in designing and implementing the backend. I worked on building RESTful APIs for tenders, bids and vendors, defining the database schema in RDS, and handling file uploads to S3. I also helped with the CI/CD pipeline and deployment on EC2. This project challenged me to learn many new tools quickly, and it gave me confidence that I can handle end-to-end backend work, from design to deployment, in a cloud environment.

Q17. What is one technical achievement in your resume that you would like to talk about more?
Answer:
One technical achievement I like to mention is building a reliable data synchronization system using Celery and Redis in the Thirdi-AI project. We had to regularly fetch marketing data from multiple external APIs with different rate limits and response formats. I helped design Celery tasks that ran on schedules, handled retries, and stored intermediate results safely. I also worked on aggregating and normalizing the data so that reports were accurate. This work significantly reduced manual effort for the team and gave the client a near real-time view of their marketing performance.

Q18. What did you learn from working on multiple projects in parallel?
Answer:
Working on multiple projects like Thirdi-AI, DHL Logistics and ProcureMate in parallel taught me how to manage time and context switching. I learned to keep clear notes about what I am doing in each project, so I can quickly resume work later. I also understood the importance of communicating priorities with my manager, so I know which tasks are most urgent. It pushed me to write cleaner code and better documentation, because I might not look at that code again for a while. Overall, it improved my discipline, planning and ability to handle pressure.

Q19. Why are you looking for a new opportunity now?
Answer:
I am grateful for the experience I gained at Bridgefix, but I feel ready for the next step. I want to work in a team where I can solve more complex backend problems, learn from strong senior engineers and work on larger-scale systems. I am also looking for a place with good code review culture, clear architecture practices and opportunities to work more deeply with cloud technologies. A new opportunity can help me grow faster in terms of both technical skills and responsibility, and allow me to contribute more to the product and the team.

Q20. What kind of backend role are you ideally looking for next?
Answer:
I am looking for a backend developer role where I can work mainly with Python, Django/DRF or FastAPI, and modern databases like PostgreSQL. I would like to be part of a product team that follows good engineering practices: version control, code reviews, testing and CI/CD. I am interested in roles where I can take end-to-end ownership of features, from understanding the requirement to deployment and monitoring. It would be great if the company uses cloud platforms like AWS or similar, so I can continue improving my skills in cloud deployments, performance optimization and scalable architectures.

Q21. How has your B.Sc. in Electronics and Communication helped you as a software engineer?
Answer:
My B.Sc. in Electronics and Communication gave me a strong base in math, logic and problem solving. Subjects like digital electronics, signals and systems and communication theory trained me to think in a structured way. I also learned how large systems are broken into smaller components that must work together correctly, which is very similar to software architecture. Lab work and mini projects taught me to debug issues step by step, read datasheets and documentation, and be patient with experiments. Even though I now work in software, this mindset of systematic thinking, patience and attention to detail helps me every day when I debug backend issues or design new features.

Q22. How did you transition from Electronics and Communication to Python backend development?
Answer:
During my degree, I realized I enjoyed coding assignments more than pure electronics work. I started learning Python on my own through online courses and simple projects. After understanding basics like variables, loops and functions, I moved to learning Django and web development. I built small personal projects and followed tutorials to create CRUD apps and simple APIs. This helped me build a portfolio and confidence. When I got my opportunity at Bridgefix, I focused on learning from seniors and real-world codebases. Over time, I shifted fully into backend development, using my problem-solving skills from E&C and combining them with Python and web technologies.

Q23. What did you learn during your degree that you still apply in software engineering?
Answer:
From my degree, I still apply several important habits. First is strong fundamentals in math and logic, which help me reason about algorithms, data structures and performance. Second is the ability to break complex problems into smaller blocks, something we did often in circuit design and communication systems. Third is careful documentation and reporting, which we practiced in lab reports and projects; now I use that skill for writing technical documentation, API specs and clean comments. Finally, working on group projects taught me teamwork, time management and communication, which are all essential in software engineering.

Q24. How did you first start learning Python?
Answer:
I first started learning Python through online tutorials and YouTube videos while I was still in college. I was attracted by how readable the language looked compared to C or C++. I began with very simple programs, like calculators and small scripts to practice loops and functions. Then I moved to solving coding problems on websites to build confidence. After that, I explored libraries for web development, especially Django. Building my first small web app, even if it was basic, motivated me a lot. Step by step, I improved by reading documentation, following tutorials and trying to build slightly more complex projects.

Q25. Did you do any coding projects or internships during your college years?
Answer:
During college, I worked on a few small coding projects, even though my main branch was Electronics and Communication. I built simple Python scripts to automate tasks like file renaming and basic data processing. I also tried small web projects using Django, such as a basic student record system. These were not large commercial projects, but they helped me understand how web applications are structured and how to connect Python code with a database and HTML templates. I did not have a big formal software internship, but these self-directed projects were very important for my learning and for getting my first job.

Q26. How did your academic performance (CGPA 7.7) reflect your learning style?
Answer:
My CGPA of 7.7 shows that I was consistent and serious about my studies, but I also focused a lot on practical learning outside the syllabus. I tried to balance time between college subjects and learning programming on my own. Sometimes this meant I did not chase the absolute highest grades, but I made sure I understood concepts well enough to use them. I learn best by doing, so I spent many hours on hands-on practice, coding exercises and small projects. This approach has helped me in my career, where real skills and problem-solving ability matter more than marks.

Q27. What non-technical skills did you gain during your education that help you in your job?
Answer:
During my education, I developed several non-technical skills that are very useful in my job now. Group projects and lab work improved my teamwork and communication, because we had to coordinate tasks, share progress and present our results. Preparing for exams and managing assignments taught me time management and planning. Giving presentations in class improved my confidence in speaking in front of others, which helps when explaining technical ideas to teammates or managers. Finally, dealing with difficult subjects and tight deadlines built resilience and patience, which are important when handling production issues or complex bugs in software projects.

Q28. How do you keep learning new technologies after graduation?
Answer:
After graduation, I keep learning by following a simple routine. I regularly read documentation, blogs and watch tutorials about tools I use, like Django, DRF, FastAPI, Celery and AWS. When I see a new concept that is relevant to my work, I try to build a small demo project or proof of concept to understand it practically. I also learn a lot from code reviews and my teammates code in real projects. Sometimes I follow online courses to deepen my knowledge in a focused way. I believe continuous learning is necessary in software, so I try to improve a little bit every week.

Q29. What motivated you to move from Ujjain to Indore (if applicable) for work?
Answer:
Moving from Ujjain to Indore was mainly a career decision. Indore has more IT companies and opportunities for software developers compared to my hometown. I wanted to work in a professional environment where I could learn from experienced engineers and work on real products. At the same time, Indore is not very far from Ujjain, so I can still visit my family when needed. The move helped me grow both personally and professionally. I learned to live independently, manage my time and finances, and adapt to a new work culture while still staying connected to my roots.

Q30. If you could redo your education, would you focus earlier on software? Why or why not?
Answer:
If I could redo my education, I would probably start focusing on software a bit earlier, especially on Python, data structures and web development. This might have given me more time to explore different areas of software and build a stronger portfolio before graduation. However, I do not regret choosing Electronics and Communication, because it gave me a good foundation in logic, systems thinking and problem solving. In the end, both paths combined have helped me become a better backend developer. So I would keep the same branch, but add more structured coding practice from the second year itself.

Q31. Can you describe your role as an Associate Software Engineer at Bridgefix Technologies?
Answer:
As an Associate Software Engineer at Bridgefix Technologies, I work mainly on backend development using Python, Django, Django REST Framework and FastAPI. My role includes designing and implementing RESTful APIs, modeling database schemas in PostgreSQL or MySQL, and building background jobs with Celery and Redis. I also help with debugging production issues, improving performance and sometimes supporting deployments with Docker and AWS. I collaborate closely with frontend developers, product managers and senior engineers to understand requirements and turn them into working features. Overall, my role is to contribute reliable, clean and maintainable backend code that supports the companys products.

Q32. What are your main daily responsibilities in your current role?
Answer:
On a daily basis, my responsibilities include reading and understanding new feature tickets, writing or updating backend code, and reviewing merge requests. I often design or modify Django models, serializers and views, or FastAPI endpoints, and implement business logic. I also write and run tests, use Postman to verify APIs, and check logs or monitoring tools when there are issues. Communication is another part of my day: I attend stand-up meetings, discuss edge cases with product managers and coordinate with frontend developers. When needed, I help investigate bugs reported by QA or clients and provide fixes or workarounds.

Q33. What types of backend modules have you developed with Django and DRF at Bridgefix?
Answer:
At Bridgefix, I have developed several backend modules using Django and DRF. These include user and role management, authentication modules, reporting and analytics APIs, shipment and logistics workflows, tender and vendor management, and background processing for data synchronization. I have worked on both simple CRUD modules and more complex features involving multiple related models, filtering, sorting and pagination. In some modules, I also implemented custom permissions, validations and business rules specific to the clients domain. Through this, I gained experience in structuring Django apps, organizing code for larger projects and keeping APIs consistent across modules.

Q34. How do you ensure a clean architecture and reusable components in your Django projects?
Answer:
To ensure clean architecture in Django projects, I try to separate concerns clearly. Models handle data structure and simple logic, while business rules are often placed in services or manager methods instead of views. Views or viewsets stay thin and focus on request handling and orchestration. I create reusable serializers and validators when the same logic is needed in multiple places. I also group related functionality into separate Django apps and use clear naming conventions. When I notice repeated patterns, I extract them into mixins, base classes or utility functions. Regular code reviews and refactoring also help maintain a clean structure over time.

Q35. Can you describe the process you follow when building a new RESTful API at Bridgefix?
Answer:
When building a new RESTful API, I start by clarifying the requirement and agreeing on the endpoint path, HTTP method and request/response format with the team, especially frontend. Then I design or update the database models if necessary. Next, I create or adjust serializers in DRF, defining which fields are exposed and how validation should work. After that, I implement viewsets or API views that connect the serializers and models, handle permissions and apply any business logic. I add URL routes, write tests or at least manual test scenarios, and then use Postman to test different cases. Finally, I clean up the code, run linters/tests and open a pull request.

Q36. How do you implement authentication and authorization in your APIs?
Answer:
In my APIs, authentication and authorization usually depend on the project requirements. With Django and DRF, I typically use token-based or JWT authentication, or session authentication for internal tools. DRFs authentication classes help verify who the user is. For authorization, I rely on DRF permission classes and custom permissions to check what the user is allowed to do. For example, I might allow only admins to access certain endpoints, or only owners to modify their own resources. In FastAPI, I use dependency injection to check tokens and roles before executing the main logic. I always try to keep security checks centralized and reusable.

Q37. What patterns do you use for role-based access control in your applications?
Answer:
For role-based access control, I usually model roles and permissions in the database. Users are linked to roles, and roles define what actions are allowed. In Django/DRF, I create custom permission classes that check the users role and sometimes additional attributes like ownership or organization. I also try to keep the permission logic consistent across endpoints by reusing the same permission classes instead of writing ad-hoc checks everywhere. Sometimes I also use groups or a simple boolean flags approach for smaller systems. The main goal is to make it easy to understand who can do what and avoid duplicating complex if-else checks in many places.

Q38. How do you handle input validation in your Django/DRF APIs?
Answer:
In Django/DRF APIs, I rely heavily on serializers for input validation. I define fields with appropriate types and constraints, like max_length, required, or choices. I also use serializer-level validation methods, such as validate_<field> or validate, to implement custom business rules, for example checking that a date range is valid or that related objects exist. This keeps validation logic close to the data structure. When validation fails, DRF automatically returns clear error messages and HTTP 400 responses. In some cases, I also add extra checks in views or services for complex cross-field validations, but I try to keep most validation centralized in serializers.

Q39. Can you give an example of a complex API you built for an admin dashboard or user role feature?
Answer:
One example of a complex API I worked on was an admin dashboard endpoint that returned combined information about users, their roles and recent activity. The endpoint needed to support filtering by role, status and date range, as well as pagination and sorting. I designed efficient queries using Django ORM with annotations and select_related to reduce database hits. I also created serializers that included nested data for user details and aggregated stats. Permissions were strict so only admins could access it. This API helped admins quickly see which users were active, which roles were assigned and where there were potential issues.

Q40. How do you test your APIs before they go to production?
Answer:
Before APIs go to production, I test them in several ways. First, I write unit tests or integration tests where possible, using Djangos test framework or pytest. These tests cover main success scenarios and important edge cases. Second, I manually test the endpoints using Postman, checking different inputs, including invalid data, missing fields and boundary values. I also verify authentication and authorization behaviour. If the project has a staging environment, I test there as well to make sure configuration and integrations work as expected. Finally, after deployment, I watch logs and monitoring tools for any unexpected errors or performance problems.
Q41. How have you used Celery and Redis for background jobs in your current role?
Answer:
In my current role, I use Celery with Redis as a message broker to move heavy or slow tasks out of the main request flow. Typical examples include sending emails, synchronizing data with external APIs, recalculating reports and generating exports. When a user triggers an action that would take time, we create a Celery task instead of doing everything in the HTTP request. Redis stores the task messages and Celery workers pick them up and process them in the background. This keeps the API responses fast and makes the system more reliable, because failed tasks can be retried without blocking users.

Q42. Can you describe a background job you implemented to improve system responsiveness?
Answer:
One background job I implemented was for syncing marketing campaign data from external APIs. Initially, the system tried to fetch fresh data every time the user opened the dashboard, which made the page slow. I moved this work into a Celery task that runs periodically, collects data and stores pre-computed results. The dashboard API then only reads from our own database, which is much faster. This change improved the user experience because dashboards loaded quickly, and it also reduced the number of calls we made to external APIs, helping us stay within rate limits.

Q43. How do you schedule periodic data sync tasks with Celery?
Answer:
To schedule periodic data sync tasks with Celery, I use Celery Beat or a similar scheduler. I define periodic tasks in the Celery configuration, specifying the task name and the schedule, such as every five minutes or hourly. For example, for marketing data sync, I define a task that loops over active accounts and fetches updates. Celery Beat sends these tasks to the Redis broker at the configured times, and Celery workers execute them. I also add logging and basic error handling inside the tasks so I can see when they run and whether they succeed or fail.

Q44. What have you done to optimize Celery tasks for performance and reliability?
Answer:
To optimize Celery tasks, I try to keep each task focused and not too heavy. If a job can be split into smaller independent pieces, I break it down so workers can process them in parallel. I avoid unnecessary database queries inside loops and use bulk operations when possible. I also configure reasonable timeouts, retries and exponential backoff for tasks that call external services. For reliability, I add clear logging and sometimes alerting on failure counts. In some cases, I use task chaining or groups to manage workflows in a structured way instead of writing complex logic in a single task.

Q45. How do you monitor and debug failed background jobs?
Answer:
To monitor and debug background jobs, I rely on logs, dashboards and sometimes admin tools like Flower or built-in Celery monitoring. I make sure each task logs key events, inputs and errors. When a task fails repeatedly, I check the logs to see the exception and context. I also look at the retry count to understand if it is a temporary external issue or a code bug. In development, I sometimes run Celery workers in the foreground to see output directly. When needed, I add more detailed logging or temporarily simplify the task to isolate the problem.

Q46. How do you use Docker in your day-to-day development at Bridgefix?
Answer:
In day-to-day development, I use Docker to create consistent environments for services like the backend, database, Redis and sometimes other dependencies. With Docker Compose, I can start the whole stack with one command, which makes onboarding and switching between projects easier. I build images for the backend service that include Python, dependencies and our code. This setup helps me ensure that the application runs the same way on my machine as in staging or production, reducing issues caused by environment differences. Docker also makes it simpler to test changes in isolation without affecting my main system.

Q47. What does a typical Docker setup look like for one of your services?
Answer:
A typical Docker setup for one of our backend services includes a Dockerfile for the application and a docker-compose.yml file for local development. The Dockerfile starts from a Python base image, copies the requirements file, installs dependencies, then copies the application code and sets the command to run the server, for example with Gunicorn or Uvicorn. The docker-compose file defines services for the backend, database (PostgreSQL or MySQL), Redis and sometimes Celery workers and beat. Each service has volumes, environment variables and port mappings. This setup allows developers to start all components with a single command.

Q48. Have you worked on setting up or maintaining CI/CD pipelines? What was your role?
Answer:
Yes, I have helped with CI/CD pipelines, especially on the ProcureMate project. My role included defining steps in the pipeline such as installing dependencies, running tests, building Docker images and deploying to staging or production environments. I worked with tools like GitHub Actions or similar CI systems to configure workflows triggered on pushes or pull requests. I also helped set up environment variables and secrets for AWS credentials and database connections. While I was not the main DevOps engineer, this experience gave me a good understanding of how automated testing and deployments work together.

Q49. How do you work with PostgreSQL or MySQL in your projects (schema design, indexing, optimization)?
Answer:
In my projects, I usually define database schemas using Django models or SQLAlchemy-style models, thinking carefully about relationships, data types and constraints. I add indexes on fields that are frequently used in filters or joins, such as foreign keys, status fields or timestamps. When I see slow queries, I check the query plan and optimize by adding or adjusting indexes, simplifying queries or using select_related and prefetch_related. I also consider using composite indexes for common filter combinations. For reporting or heavy reads, I sometimes denormalize or add summary tables if needed, while still keeping data integrity.

Q50. How do you collaborate with product managers and frontend teams at Bridgefix?
Answer:
I collaborate with product managers by discussing requirements, clarifying edge cases and estimating effort for backend tasks. We often refine user stories together so that technical constraints are understood. With frontend teams, I work closely on API design, agreeing on endpoints and response formats. I share Postman collections or API docs and stay available for quick questions on chat or calls. During development, I provide test accounts or sample data to help them integrate. If we find mismatches or issues, we adjust the API or UI together. Good communication with both sides helps us deliver features smoothly and reduce rework.

Q51. What is Thirdi-AI and what problem does it solve?
Answer:
Thirdi-AI is a marketing analytics platform that connects to different advertising channels like Facebook, Google, TikTok and LinkedIn. Its main goal is to pull campaign data from these platforms, combine it in one place and show clear reports to clients. Instead of logging into each ad platform separately, users can see impressions, clicks, conversions and costs together in a single dashboard. This helps marketing teams understand which campaigns are performing well, where they are spending money and how to optimize their budgets. In short, Thirdi-AI solves the problem of scattered marketing data by centralizing and standardizing it.

Q52. What was your specific role on the Thirdi-AI project?
Answer:
On the Thirdi-AI project, my role was to develop and maintain backend services using Python, Django and Django REST Framework. I helped integrate external marketing APIs, design database models for campaigns and metrics, and implement Celery tasks for scheduled data synchronization. I also worked on building RESTful APIs that powered the reporting dashboards and filtering features used by the frontend. In addition, I was involved in debugging issues with API responses, handling rate limits and improving performance of data aggregation queries. Overall, I contributed to making the data pipeline reliable and the APIs easy to use.

Q53. How did you integrate marketing APIs like Facebook, Google, TikTok, and LinkedIn?
Answer:
To integrate marketing APIs, I first studied each platforms documentation to understand their authentication method, endpoints and rate limits. I then wrote Python client functions or small wrapper classes to call these APIs using the official SDKs or HTTP requests. For each platform, I implemented logic to fetch campaign, ad set and ad-level metrics, often using pagination. Responses were parsed and mapped into our internal data structures. I also added proper error handling, logging and retry mechanisms for temporary failures. Finally, these integration functions were called from Celery tasks on a schedule so that data was kept up to date without blocking user requests.

Q54. How did you handle authentication and security when calling these external marketing APIs?
Answer:
Authentication depended on each provider. For many marketing APIs, we used OAuth tokens or access tokens generated through their developer portals. I stored these tokens securely in the database or environment variables, never in plain text in code. When calling the APIs, the token was added to the request headers as required. I also implemented token refresh flows where needed, for example when long-lived and short-lived tokens were involved. All communication with external APIs was done over HTTPS, and sensitive configuration values were managed via environment variables and not committed to the repository.

Q55. What data did you retrieve from these marketing platforms (impressions, clicks, conversions, costs, etc.)?
Answer:
From the marketing platforms, we typically retrieved campaign metadata and performance metrics. This included impressions, clicks, conversions, cost, click-through rate, cost per click and cost per conversion. We also fetched information about campaigns, ad sets and ads, such as names, objectives, budgets and statuses. In some cases, we collected breakdowns by device, location or placement. This raw data was then stored in our database and later aggregated or filtered to build reports. By combining similar metrics across platforms, we allowed users to compare performance across different channels in a unified view.

Q56. How did you design the data aggregation logic to normalize metrics from different sources?
Answer:
Designing data aggregation logic started with identifying common metrics across all platforms, such as impressions, clicks, conversions and spend. I created a standard internal schema where each record represented performance for a campaign or ad over a time period, with fields for these metrics. For each platform, we mapped their specific field names to this standard schema. Then, aggregation queries summed or averaged values by campaign, date range or channel. Any platform-specific metrics were stored separately but could still be used for advanced reports. This normalization allowed us to show combined performance charts even though the original APIs were different.

Q57. How did you model the data in the database for campaigns and metrics in Thirdi-AI?
Answer:
In the database, I modeled separate tables for accounts, campaigns, ad sets, ads and daily performance metrics. Each campaign or ad had foreign keys linking it to its parent account and related objects. A metrics table stored numerical values like impressions, clicks and cost, along with references to the campaign or ad and the date. This design allowed us to run queries by account, campaign, time period or channel. Indexes were added on foreign keys and date fields to make aggregations and filtering faster, especially for dashboards that showed data over long time ranges.

Q58. What challenges did you face when aggregating data from multiple APIs with different schemas?
Answer:
One challenge was that each platform used slightly different field names, data types and definitions. For example, what counted as a "conversion" could differ between providers. To handle this, we carefully mapped fields to a shared internal meaning and documented any differences. Another challenge was handling missing or partial data when an API call failed or a platform had outages. Time zones and reporting windows also varied, so we had to convert everything to a common time standard. Finally, performance was a challenge when aggregating large datasets, so we had to optimize queries and sometimes precompute summaries.

Q59. How did you use Celery and Redis to schedule periodic data sync for Thirdi-AI?
Answer:
For periodic data sync in Thirdi-AI, I used Celery workers with Redis as a broker, and Celery Beat or a scheduler to trigger tasks. I defined tasks that, for each connected account, fetched new or updated campaign data from the external APIs. These tasks ran at intervals such as every hour or every few minutes, depending on the clients needs and API limits. Redis held the queue of tasks, and Celery workers processed them in the background. This setup ensured that data was refreshed regularly without blocking user requests or overloading the APIs.

Q60. How did you ensure that data sync jobs did not overload the external APIs or your own system?
Answer:
To avoid overloading external APIs, I respected their rate limits by spacing out requests and using pagination carefully. In some cases, I added small delays or limited the number of concurrent tasks per account. For our own system, I scheduled heavy sync jobs during periods of lower user activity when possible. I also made sure database writes were done efficiently, using bulk operations instead of inserting row by row. Monitoring and logging helped me detect when jobs were taking too long or failing frequently, so I could adjust schedules or optimize code.
Q61. How did you handle failures or rate limits from third-party marketing APIs?
Answer:
When calling third-party marketing APIs, I expected that failures and rate limits would happen. I added try/except blocks around API calls and logged detailed error messages, including the endpoint and parameters. For temporary errors or timeouts, I used Celerys retry mechanism with exponential backoff, so tasks would automatically try again later. When rate limits were reached, I respected the headers provided by the API, such as retry-after values, and paused or slowed down further requests. I also made sure our system handled partial data gracefully, so one failed account did not break reporting for others.

Q62. How did you test and validate that the aggregated metrics were correct?
Answer:
To validate aggregated metrics, I used both automated checks and manual comparisons. For a few sample campaigns, I manually downloaded or viewed reports directly in the marketing platforms and compared them to our dashboard values for the same date range. I also wrote small scripts or queries to verify sums and averages at different levels, such as per campaign and per day. During development, we used staging accounts and test data to confirm that mappings and conversions were correct. If there were differences, I traced them back to the raw API responses and our transformations until I understood and fixed the issue.

Q63. What kind of API endpoints or dashboards were built on top of this aggregated data?
Answer:
On top of the aggregated data, we built several API endpoints for dashboards and reports. Examples include endpoints to list campaigns with key metrics, get time-series data for charts, compare performance across channels, and filter by date range, account or campaign status. The frontend used these APIs to draw graphs like daily spend, clicks and conversions, and to show tables with sortable columns. Some endpoints also supported exporting data to CSV for further analysis. The goal was to give marketing teams a clear, flexible view of their performance across different platforms.

Q64. How did you structure your Django/DRF codebase for the Thirdi-AI project?
Answer:
In Thirdi-AI, the Django/DRF codebase was structured into separate apps for accounts, integrations, campaigns, metrics and reporting. Each app contained its own models, serializers, views and URLs. Shared utilities, such as API client wrappers and common validators, were placed in a utils or services module. I used DRF viewsets and routers for many endpoints, which kept URL definitions clean. Business logic that did not belong directly in views or models was moved into service functions or classes. This modular structure made it easier to work on one part of the system without affecting others too much.

Q65. Did you have to deal with time zones and reporting periods when aggregating campaign data?
Answer:
Yes, time zones and reporting periods were important. Different platforms sometimes reported data in different time zones or allowed custom reporting windows. To keep things consistent, we usually converted all timestamps to UTC in the database and stored the original time zone information separately if needed. When aggregating data by day, we defined a clear rule for what day meant, usually based on the clients local time zone. This required careful handling when querying and grouping data, but it ensured that reports matched the clients expectations.

Q66. How did you handle large volumes of historical data in this project?
Answer:
Handling large volumes of historical data required both careful schema design and query optimization. I made sure tables had proper indexes on foreign keys and date fields, and I avoided unnecessary joins in frequent queries. For very old data that was rarely accessed, we considered archiving strategies or limiting queries to recent periods by default, with options to load more when needed. In some cases, we precomputed summary tables for common reports, which reduced the need to scan raw data repeatedly. Regular maintenance tasks, like vacuuming and analyzing the database, also helped keep performance stable.

Q67. What performance optimizations did you implement in Thirdi-AI?
Answer:
Some key performance optimizations in Thirdi-AI included reducing N+1 query problems by using select_related and prefetch_related in Django ORM, adding the right indexes to frequently filtered fields, and caching some heavy computations when possible. I also moved expensive work such as data fetching and aggregation into background Celery tasks, so user-facing APIs mainly read from prepared data. For certain reports, I created denormalized tables or materialized-like structures to speed up complex aggregations. These changes helped the dashboards load faster and reduced CPU and database load under higher traffic.

Q68. If you had more time, what improvements would you make to Thirdi-AI?
Answer:
If I had more time, I would improve Thirdi-AI by adding more robust monitoring and alerting specifically for data freshness and quality. For example, dashboards could show when data was last synced and warn if a particular integration had repeated failures. I would also work on a better UI for managing integration settings and tokens, making it easier for users to reconnect accounts. On the technical side, I would consider using a data warehouse or columnar storage for very large datasets and more advanced analytics, to further improve performance and flexibility.

Q69. What did you personally learn from working on Thirdi-AI?
Answer:
Working on Thirdi-AI taught me a lot about integrating with third-party APIs, dealing with imperfect data and designing resilient data pipelines. I learned how to read and understand complex API documentation, handle rate limits and errors gracefully, and design schemas that support flexible reporting. I also improved my skills in Celery, Redis and performance optimization. On the softer side, I learned how important clear communication is when explaining data discrepancies to non-technical team members and how to set the right expectations around data synchronization and reporting delays.

Q70. How would you explain the Thirdi-AI project to a non-technical client?
Answer:
To a non-technical client, I would say: Thirdi-AI is a tool that connects to all your online advertising accounts, like Facebook Ads and Google Ads, and brings their data into one place. Instead of checking each platform separately, you get a single dashboard where you can see how much you spent, how many people saw your ads, how many clicked and how many converted. The system automatically updates this information in the background, so you always see fresh data. This helps you quickly understand which campaigns are working and where you should adjust your budget to get better results.

Q71. What is the DHL Logistics Platform project about?
Answer:
The DHL Logistics Platform project is about managing shipments and logistics workflows in a structured, digital way. The system allows users to create shipments, assign routes, track status updates and manage related data such as senders, receivers and carriers. Different user roles, such as admins, operators and clients, can see and act on the data that is relevant to them. The goal is to make shipment tracking more transparent and efficient, reduce manual work and errors, and provide clear visibility into where packages are and what actions are pending at each step in the logistics chain.

Q72. What was your role and responsibility in the DHL Logistics Platform project?
Answer:
In the DHL Logistics Platform project, my role was to work on backend services using Django, DRF and PostgreSQL. I helped design and implement APIs for creating and updating shipments, managing user roles and permissions, and tracking shipment status. I also worked on background tasks with Celery for time-sensitive operations, such as sending alerts or recalculating routes. Additionally, I contributed to the Docker setup, ensuring services ran in containers with proper user permissions. I was responsible for writing clean, tested code, fixing bugs and collaborating with frontend and product teams to refine features.

Q73. How did you design backend services for logistics workflows and shipment tracking?
Answer:
When designing backend services for logistics workflows, I started by understanding the real-world process: how shipments are created, scheduled, picked up, transported and delivered. I then modeled these steps as states in the system, with clear transitions between them. For each part of the workflow, I designed APIs that allowed creating or updating shipments, assigning routes, and recording events like pickup or delivery. I ensured that each transition checked the current status and user permissions to prevent invalid operations. Tracking data, such as timestamps and locations, was stored so we could build accurate views of each shipments history.

Q74. What kind of data models did you define for shipments, users, and roles?
Answer:
For shipments, I defined models with fields for tracking number, origin, destination, current status, assigned route, and timestamps for key events like created, picked up and delivered. For users, I included basic profile information and relationships to roles or permissions. Roles defined what actions a user could perform, such as creating shipments, updating statuses or viewing reports. There were also models for related entities like addresses, carriers and route segments. Relationships between these models were handled with foreign keys and many-to-many relationships where needed, which allowed flexible queries and permissions checks.

Q75. How did you manage shipment status updates and tracking information?
Answer:
Shipment status updates were managed through dedicated API endpoints that allowed authorized users or systems to record events, such as created, picked up, in transit or delivered. Each update changed the shipments status and stored a timestamp and sometimes location data. I ensured that the system only allowed valid transitions, for example you cannot mark a shipment as delivered before it is in transit. Tracking information was stored as a history table or related model, so we could show a timeline of events for each shipment. This data was used in dashboards and notifications for customers and internal users.

Q76. How did you implement user role management in this project?
Answer:
User role management was implemented using separate models for users and roles, with a many-to-many relationship between them. Each role represented a set of permissions, such as admin, operator, or client. In the Django/DRF layer, I wrote custom permission classes that checked the logged-in users roles before allowing certain actions, like editing shipments or accessing specific reports. Some endpoints were restricted to admins only, while others allowed operators or clients with limited access. This approach made it easier to change role definitions without rewriting permission logic everywhere.

Q77. How did you design REST APIs for creating, updating, and viewing shipments?
Answer:
I designed REST APIs for shipments following standard patterns. For creating shipments, there was a POST endpoint that accepted JSON data with sender, receiver, address and package details. For updating shipments, such as changing status or route, there were PATCH or PUT endpoints with appropriate validation. For viewing shipments, GET endpoints supported listing with filters (by status, date, user, etc.) and retrieving detailed information for a single shipment. Responses included both core shipment data and related information like current status, last event and key timestamps. Pagination and sorting were added to keep list responses efficient.

Q78. What were the main challenges when building APIs for logistics workflows?
Answer:
Some main challenges when building logistics APIs were handling complex business rules and many edge cases. For example, not all shipments follow the exact same path; some are returned, cancelled or delayed. The APIs had to prevent invalid state changes and handle exceptions gracefully. Performance was also important when listing or searching large numbers of shipments with filters. Another challenge was coordinating changes between different user roles and systems, such as internal operators and external partners, while keeping data consistent. Clear logging and monitoring were necessary to debug issues in multi-step workflows.

Q79. How did you use Docker for this project, especially for handling root and non-root users?
Answer:
In this project, Docker was used to containerize the backend service, database and other components. We paid attention to running containers as non-root users for better security. In the Dockerfile, instead of using the default root user, we created a dedicated user, set proper ownership on application directories and switched to that user before running the application. This reduced the risk that a security issue in the app could be used to gain root access on the host. Docker Compose configurations were updated accordingly so that volumes and ports still worked correctly with the non-root user.

Q80. Why is running containers as non-root users important, and how did you configure it?
Answer:
Running containers as non-root users is important because it limits the damage that can be done if the application is compromised. If a process inside the container runs as root, an attacker may be able to escape the container and affect the host system. By using a non-root user, we follow the principle of least privilege. To configure this, we created a user in the Dockerfile, changed ownership of working directories to that user and used the USER instruction to switch context. We also adjusted file permissions and volume mounts so the application could still read and write what it needed without root access.
Q81. How did you use Celery and Redis in the DHL Logistics Platform?
Answer:
In the DHL Logistics Platform, Celery and Redis were used to handle tasks that should not block API responses, especially time-sensitive or repetitive operations. Examples include sending notifications when a shipment status changed, recalculating routes based on new information, and syncing data with external systems. Redis acted as the broker where tasks were queued, and Celery workers processed them in the background. This allowed the main web application to respond quickly while still ensuring important operations happened reliably and could be retried if they failed.

Q82. Can you describe a background task related to shipment updates or alerts that you implemented?
Answer:
One background task I worked on sent alerts when a shipment was delayed or stuck in the same status for too long. The task periodically scanned shipments, checked their last update time and compared it to expected timelines. If a shipment exceeded the threshold, the task created an alert record and sometimes triggered an email or notification to the responsible user. Running this as a Celery task allowed the checks to happen regularly without slowing down normal user actions, and we could adjust the schedule or thresholds based on business needs.

Q83. How did you ensure that time-sensitive operations (like alerts and route recalculations) were handled reliably?
Answer:
To handle time-sensitive operations reliably, I made sure tasks were idempotent and could be retried safely. Tasks logged their progress and any errors, so we could monitor their health. I configured Celery with appropriate retry policies and time limits, to avoid stuck tasks. For critical operations, we sometimes used additional checks, such as verifying that a shipment had not already been updated before applying a change. We also tested these workflows thoroughly in staging, simulating delayed shipments and route changes to make sure the alerts and recalculations behaved as expected.

Q84. How did PostgreSQL help you handle logistics data efficiently in this project?
Answer:
PostgreSQL was very useful for handling logistics data because of its strong support for relational data and indexing. I designed normalized tables for shipments, routes, users and events, and created indexes on fields commonly used in filters, such as status, dates and foreign keys. PostgreSQLs query planner, combined with proper indexes, allowed fast searches and sorts even when dealing with many shipments. Features like transactions ensured data integrity when multiple updates happened together. In some cases, I used advanced features such as JSON fields or partial indexes to optimize specific queries.

Q85. How did you ensure data consistency when many updates were happening at the same time?
Answer:
To ensure data consistency with many concurrent updates, I relied on database transactions and careful locking behaviour. In Django, I used atomic blocks where multiple related operations had to succeed or fail together. I also tried to keep transactions short to reduce contention. For certain critical updates, I used select_for_update or similar mechanisms to avoid race conditions. At the application level, I validated state transitions, so two conflicting updates would not push a shipment into an invalid status. Combined with good indexing and error handling, this kept data consistent even under concurrent load.

Q86. How did you test the logistics workflows end to end?
Answer:
End-to-end testing of logistics workflows involved both automated tests and manual scenarios. I wrote integration tests that simulated creating shipments, assigning routes, updating statuses and generating alerts, checking that the system behaved as expected at each step. These tests used the same APIs as the frontend, ensuring real behaviour. In addition, we created test data and walked through workflows manually in a staging environment, verifying that dashboards, notifications and background tasks all worked together. We also tested edge cases, like cancelled shipments or repeated status changes, to make sure the system was robust.

Q87. What were some edge cases you had to handle in shipment tracking features?
Answer:
Some edge cases in shipment tracking included shipments that were returned to sender, cancelled at different stages, or delayed for long periods. We had to ensure that the system handled these states gracefully, updating statuses and notifications correctly. Another edge case was out-of-order updates, where events arrived later than expected; we needed to handle these without corrupting the history. Partial data from external systems or missing location information also had to be handled so the UI did not break. Designing clear rules for each special case helped keep the system predictable.

Q88. How did you log and monitor the system in the DHL Logistics project?
Answer:
For logging and monitoring, we used structured logs from the backend application, including important events like status changes, errors and background task results. Logs were centralized so we could search them easily when debugging. We also used metrics and dashboards to track key indicators such as the number of shipments processed, task failures and response times. Alerts were configured for abnormal conditions, like spikes in errors or long response times. This combination of logging and monitoring helped us quickly detect issues in the logistics workflows and fix them before they affected many users.

Q89. What did you learn from working with a logistics domain?
Answer:
Working in the logistics domain taught me how important accurate, real-time data is for operations and customer satisfaction. I saw how small delays or incorrect statuses could create confusion for many people. I learned to think in terms of physical flows (packages moving through locations) and mirror them correctly in the data model and APIs. The domain also made me more careful about designing clear status transitions and handling exceptions like returns or cancellations. Overall, it improved my ability to translate complex real-world processes into reliable backend systems.

Q90. If you could redesign one part of the DHL Logistics Platform, what would you change?
Answer:
If I could redesign one part of the DHL Logistics Platform, I would focus on making the event and status tracking system even more flexible and extensible. I would consider using a more explicit event-sourcing style, where every change is recorded as an immutable event, and current status is derived from these events. This would make it easier to audit history, replay scenarios and support new workflows without changing core logic. I would also invest in better tools for visualizing shipment journeys, so operations teams can quickly understand where delays occur and why.

Q91. What is ProcureMate and what problem does it solve?
Answer:
ProcureMate is a cloud-based procurement and tender management platform built with FastAPI and AWS. It helps organizations publish tenders, onboard vendors, collect bids and manage related documents in a structured way. Instead of handling these processes through spreadsheets and email, ProcureMate centralizes everything in one system. Vendors can register, view available tenders and submit bids online, while buyers can review, compare and track responses more easily. The platform aims to make tender processes more transparent, efficient and secure, reducing manual work and the risk of losing important documents or missing deadlines.

Q92. Why was FastAPI chosen for the ProcureMate backend instead of Django or DRF?
Answer:
FastAPI was chosen for ProcureMate because of its speed, modern design and great support for asynchronous operations and automatic API documentation. Its use of Python type hints and Pydantic models made it easy to define request and response schemas with built-in validation. FastAPI also provides automatic OpenAPI/Swagger docs, which helped frontend developers and stakeholders understand the APIs quickly. For a greenfield project focused mainly on APIs rather than server-side HTML, FastAPI was a good match. It allowed us to build clean, performant endpoints while still leveraging familiar Python tooling and deployment patterns.

Q93. What was your role in building ProcureMate’s APIs?
Answer:
In ProcureMate, I worked on designing and implementing many of the backend APIs using FastAPI. My responsibilities included defining Pydantic models for requests and responses, implementing route handlers for vendor onboarding, tender creation, bid submission and document management, and connecting these handlers to the AWS RDS database. I also worked on authentication and permission checks, ensuring that only authorized users could view or modify sensitive data. Additionally, I helped integrate AWS S3 for file storage and contributed to configuring the deployment on EC2 and the CI/CD pipeline. Overall, I was involved from design to deployment for key backend features.

Q94. How did you design the RESTful APIs for vendor onboarding, tender listing, and bid submission?
Answer:
For vendor onboarding, I designed APIs that allowed vendors to register, update their profiles and upload required documents. These were standard POST, GET and PUT/PATCH endpoints with appropriate validation. For tender listing, I created endpoints that returned available tenders with filters for status, category or deadlines, including pagination. For bid submission, I implemented secure POST endpoints where vendors could submit offers and documents linked to a specific tender. Each endpoint followed RESTful principles, used clear resource-based URLs and returned meaningful status codes and error messages. Permissions were carefully checked so vendors only accessed their own data and allowed tenders.

Q95. How did you structure your FastAPI application (routers, models, schemas, services)?
Answer:
The FastAPI application was structured to keep concerns separated. I used routers to group endpoints by feature, such as /auth, /vendors, /tenders and /bids. Pydantic schemas lived in a schemas module, defining request and response models independent of the database layer. Database models and CRUD operations were in a separate models or repository layer, often using SQLAlchemy. Business logic that combined multiple operations was placed in service functions or classes. Dependencies, such as database sessions or current user checks, were implemented using FastAPIs dependency injection system. This structure made the codebase easier to understand and extend.

Q96. How did you define the database schema in AWS RDS for tenders, bids, vendors, and documents?
Answer:
In AWS RDS, I defined tables for vendors, tenders, bids and documents using relational modeling. Vendors had fields like name, contact details and status. Tenders stored information such as title, description, deadlines and owner organization. Bids linked to both a vendor and a tender, with fields for price, terms and submission timestamps. Documents were associated with either vendors, tenders or bids via foreign keys and stored metadata like file name, S3 key and type. I added indexes on foreign keys and commonly queried fields, such as tender status and deadlines, to keep queries efficient.

Q97. How did you ensure relationships between tenders, bids, and vendors were correctly modeled?
Answer:
To ensure relationships were correct, I carefully defined foreign key constraints in the schema: each bid referenced exactly one tender and one vendor, and tenders referenced their owners. I used SQLAlchemy or similar ORM relationships to model these links in code, allowing easy navigation between objects. Validation logic ensured that a vendor could not bid on closed tenders or submit duplicate bids where not allowed. Cascading behavior was considered, for example preventing deletion of tenders that still had active bids. Database constraints and application-level checks together kept relationships consistent and prevented invalid states.

Q98. How did you handle file uploads to AWS S3 in the ProcureMate project?
Answer:
For file uploads, I created FastAPI endpoints that accepted multipart/form-data requests containing files and related metadata. In the route handler, I read the uploaded file stream and used the AWS SDK (boto3) to upload it to an S3 bucket under a structured key path, often including tender or vendor IDs. After a successful upload, I stored the file metadata in the database, including the S3 key, original filename, size and type. Access to these files was controlled using signed URLs or restricted bucket policies, so only authorized users could download sensitive documents.

Q99. How did you manage permissions and security for sensitive tender and bid data?
Answer:
Permissions were managed using authentication tokens and role or ownership checks in FastAPI dependencies. Each request went through a dependency that decoded the token, fetched the user and determined their role. For endpoints dealing with tenders and bids, I checked whether the user was the tender owner, an authorized evaluator or a vendor associated with that bid. Sensitive data, like bid amounts before opening, was hidden from unauthorized users. All communication with the API and S3 used HTTPS, and secrets like database passwords and AWS keys were stored securely in environment variables or secret managers, not in code.

Q100. What were the biggest challenges you faced while working with AWS S3, EC2, and RDS?
Answer:
Some of the biggest challenges were configuring security and networking correctly across S3, EC2 and RDS. I had to ensure that EC2 instances could access RDS and S3 securely without exposing services to the public internet more than necessary. Managing IAM roles and policies for least-privilege access took careful thought. Another challenge was handling performance and cost: choosing appropriate instance sizes and storage options for RDS, and configuring connection pooling to avoid exhausting database connections. Debugging issues in the cloud environment, where logs and access are different from local development, also required patience and good monitoring.
Q101. How did you configure FastAPI to run on AWS EC2 in production?
Answer:
To run FastAPI on AWS EC2, I created a Docker image containing the application code, dependencies and a production server like Uvicorn with Gunicorn. On the EC2 instance, Docker and Docker Compose were installed, and environment variables were set for database and AWS credentials. The container exposed the application on an internal port, and an Nginx or load balancer in front handled HTTPS termination and routed traffic to the FastAPI service. I also configured system services or scripts to restart containers automatically on reboot and used security groups to restrict access to only necessary ports.

Q102. Did you use any web server like Uvicorn or Gunicorn with FastAPI? How did you configure it?
Answer:
Yes, we used Uvicorn with Gunicorn as the ASGI server for FastAPI in production. Gunicorn managed multiple worker processes, and Uvicorn workers served the FastAPI app. In the Dockerfile or entrypoint script, we ran a command like gunicorn -k uvicorn.workers.UvicornWorker app.main:app -w 4 -b 0.0.0.0:8000. The number of workers and other settings were tuned based on CPU and memory resources. This setup provided good performance and stability, and it is a common production pattern for FastAPI applications.

Q103. How did you design the CI/CD pipeline for ProcureMate deployments?
Answer:
The CI/CD pipeline for ProcureMate included steps for building, testing and deploying the application automatically. On each push or pull request, the CI system (such as GitHub Actions) installed dependencies, ran unit tests and linting, and built a Docker image if tests passed. For main branch changes, the pipeline pushed the image to a container registry and then triggered a deployment step on the EC2 server, pulling the new image and restarting the service using Docker Compose. Secrets like AWS credentials and database URLs were stored securely in the CI system and not hardcoded.

Q104. How did you ensure zero or minimal downtime during deployments?
Answer:
To ensure minimal downtime, deployments were designed to be quick and, where possible, rolling. Docker Compose or deployment scripts pulled the new image, started new containers and then stopped the old ones. Health checks were used to confirm that the new version was running correctly before fully switching traffic. Database migrations were carefully planned and applied in a way that did not break compatibility with the running code, for example by adding new columns before using them. In some cases, deployments were scheduled during low-traffic periods to reduce the impact of any small interruptions.

Q105. How did you test the ProcureMate APIs, and what tools did you use?
Answer:
I tested ProcureMate APIs using both automated tests and manual tools. Automated tests were written in Python using pytest or FastAPIs TestClient to simulate requests and verify responses. These tests covered main success and error scenarios for critical endpoints. For manual testing, I used Postman to send requests with different payloads and authentication tokens, checking responses and error handling. In staging, end-to-end tests were performed with real AWS services to make sure integration with S3 and RDS worked correctly. This combination of tests helped catch issues early and gave confidence before deploying.

Q106. How did you handle performance and scalability concerns in ProcureMate?
Answer:
Performance and scalability were handled by designing efficient database queries, using proper indexes and keeping endpoints focused. I avoided loading unnecessary data and used pagination for list endpoints. The FastAPI app was run behind Gunicorn with multiple worker processes to utilize CPU cores. Docker and AWS allowed horizontal scaling by running more containers or larger instances if needed. For heavy operations, we considered background processing instead of doing everything in request/response cycles. Monitoring tools were used to track response times and resource usage, so we could detect bottlenecks and optimize further.

Q107. How did you implement validation and error handling in FastAPI using Pydantic models?
Answer:
In FastAPI, I defined request and response models using Pydantic classes, setting proper field types, constraints and default values. FastAPI automatically validated incoming JSON against these models, returning clear error messages when data was missing or invalid. For more complex rules, I used Pydantic validators or additional checks inside the route handlers. Error handling was done using FastAPIs exception handlers to return consistent JSON error responses with appropriate HTTP status codes. This approach kept validation close to the data definitions and made APIs easier to use and debug.

Q108. What did you learn from working on a cloud-based project like ProcureMate?
Answer:
Working on ProcureMate taught me how to think beyond just code and consider the full deployment environment. I learned how EC2, S3 and RDS work together, how to configure security groups and IAM roles, and how to manage environment-specific settings. I also improved my understanding of Docker, CI/CD pipelines and monitoring in the cloud. This experience showed me the importance of designing applications that are cloud-friendly: stateless services, proper configuration management and good logging. Overall, it made me more confident in building and running backend systems in real production environments.

Q109. How would you explain ProcureMate to a non-technical stakeholder?
Answer:
To a non-technical stakeholder, I would say: ProcureMate is an online system that helps your organization manage tenders and vendor bids in one place. Instead of using email and spreadsheets, you publish tenders on the platform, vendors register and submit their offers there, and you can review and compare bids more easily. All documents and communication are stored securely, and deadlines and statuses are clearly visible. This reduces manual work, lowers the risk of mistakes and makes the tender process more transparent and auditable.

Q110. If you had to add an AI feature to ProcureMate, what would you build and how?
Answer:
If I added an AI feature to ProcureMate, I would build a recommendation or scoring system for bids. The system could analyze past tenders, winning bids and vendor performance to suggest which bids are most likely to be successful or offer the best value. Technically, I would collect historical data, engineer features like price, delivery time, quality ratings and vendor reliability, and train a machine learning model. This model could then provide a score or ranking for new bids. The AI feature would not replace human decisions but would support buyers by highlighting promising options.

Q111. What features of Python do you use most often in your backend projects?
Answer:
In backend projects I use Python features that make code clear and productive. I use functions and classes to organize logic, and modules and packages to keep the project structure clean. I rely heavily on dictionaries and lists for working with JSON data, and comprehensions to transform data in a readable way. I also use context managers (the "with" statement) for working with files, database sessions or external resources safely. Type hints are another feature I use more now, especially with FastAPI and Pydantic, because they make the code easier to understand and help with auto-completion and validation.

Q112. How do you organize Python packages and modules in a typical backend project?
Answer:
In a typical backend project, I organize code by feature or layer. At the top level there are packages like api, services, models, schemas and tests. Inside api, I have modules or subpackages for each area, such as users, tenders or shipments. Models contain database models, schemas contain Pydantic or serializer classes, and services include business logic that should not depend too much on the framework. This structure keeps files smaller and responsibilities clear. It also makes it easier for new team members to find what they need and for me to add new features without creating a big mess.

Q113. Can you explain the difference between lists, tuples, and sets in Python and when you use each?
Answer:
Lists, tuples and sets are all collections but have different purposes. A list is an ordered, changeable collection that can contain duplicates; I use lists when I need to keep items in order and update or append items. A tuple is ordered but immutable, meaning it cannot be changed after creation; I use tuples for fixed data, like coordinates or constant pairs, where I want to show that the data should not change. A set is an unordered collection of unique elements; I use sets when I care about membership tests and uniqueness, for example to remove duplicates or to check if an item appears in a group.

Q114. How do you handle errors and exceptions in your backend code?
Answer:
I handle errors by using try/except blocks around code that can fail, such as external API calls, database operations or file handling. I catch specific exceptions where possible and return clear error messages to the client with appropriate HTTP status codes, like 400 for bad requests or 500 for server errors. I avoid catching all exceptions silently, because that hides problems. Instead, I log the full stack trace using Pythons logging module so I can debug later. I also use validation at the input layer to prevent invalid data from reaching deeper parts of the system, which reduces runtime errors.
Q115. What is the difference between synchronous and asynchronous code in Python?
Answer:
Synchronous code runs one step at a time; each operation must finish before the next one starts. This is simple to understand but can be slow when waiting for network or disk operations. Asynchronous code, using async/await, allows Python to start an operation, like an API call, and switch to other work while it waits for a response. This is very useful for servers that handle many requests at the same time. Async does not make CPU-heavy code faster, but it helps with I/O-bound tasks by better using waiting time and increasing overall throughput.

Q116. Have you written any asynchronous code with async/await in FastAPI or other frameworks?
Answer:
Yes, I have written async endpoints in FastAPI. In FastAPI, you can define route handlers with the async keyword, and then use await when calling async functions, such as database or HTTP client operations that support async. This allows the server to handle other requests while waiting for these operations. I have used this pattern for endpoints that perform multiple external calls or slower I/O work, to improve responsiveness under load. I still choose async carefully and make sure that the libraries I use are truly asynchronous to get real benefits.

Q117. How do you manage virtual environments and dependencies for your Python projects?
Answer:
I manage virtual environments using tools like venv or virtualenv, sometimes combined with pip-tools or poetry. For each project, I create a separate environment and install dependencies there, keeping them isolated from other projects and the system Python. I store main dependencies in a requirements.txt or pyproject configuration and pin versions as needed for stability. In CI and Docker, I use the same dependency definitions to reproduce the environment. This approach avoids version conflicts and makes it easier to set up the project on new machines.

Q118. How do you use type hints in Python, and what benefits have you seen?
Answer:
I use type hints by adding type information to function arguments, return values and variables, like def add(a: int, b: int) -> int. In FastAPI and Pydantic, type hints are very helpful because they drive request validation and documentation. Even in normal Python code, type hints improve readability and make IDE auto-completion and static analysis tools more powerful. They catch some bugs earlier, such as passing the wrong type to a function. While Python remains dynamic, type hints give a light layer of safety and self-documentation.

Q119. How do you structure a Python project to keep business logic separate from framework code?
Answer:
To separate business logic from framework code, I try to keep core rules and calculations in plain Python modules or service classes, not inside views or route handlers. Framework-specific layers, like Django views or FastAPI routers, are thin: they handle HTTP, validation and calling the right service functions. This makes it easier to test business logic without needing a full web environment and allows reusing the same logic in different contexts, such as CLI tools or background workers. It also helps if we ever need to change the framework or add another interface.

Q120. What Python best practices do you follow to keep your code clean and readable?
Answer:
To keep my code clean, I follow simple best practices. I use meaningful names for variables and functions, and I keep functions small and focused on one task. I follow PEP 8 style guidelines for formatting, such as spacing and line length, often using tools like black or flake8. I avoid deep nesting and repeat code; instead, I extract common logic into helper functions. I write docstrings or comments where needed, especially for complex parts. Finally, I keep tests updated, because they also serve as examples of how the code should be used.

Q121. How do you decide when to create a class versus using simple functions in Python?
Answer:
I prefer to start with simple functions, because they are easy to read and test. I decide to create a class when I see that some data and behaviour belong together and are used in many places. For example, if I need to manage a complex concept like a shipment service or tender service with several related operations and shared state, a class makes sense. Classes are also useful when I need to implement clear interfaces or use inheritance. If something is just a small calculation or transformation, a function is enough and keeps the code simpler.

Q122. Can you explain list comprehensions and give an example from your work?
Answer:
A list comprehension is a short and readable way to create a new list from an existing iterable in Python. The basic form is [expression for item in iterable if condition]. I use them often when transforming data from database queries or external APIs. For example, if I get a list of campaign objects and I only need their IDs, I might write [c.id for c in campaigns]. If I want only active campaigns, I can add a condition: [c.id for c in campaigns if c.is_active]. This is more compact and clearer than writing a full for-loop to build the list.

Q123. How do you use generators in Python, and why might they be useful in backend systems?
Answer:
Generators are a way to produce values one by one instead of creating a full list in memory. I use them when working with large datasets or streams of data, so I do not load everything at once. In backend systems, generators can be useful for reading large files line by line, streaming responses or processing long sequences of records. Using the yield keyword, a function becomes a generator and returns values lazily. This reduces memory usage and can improve performance, especially when you only need to iterate once over the data.

Q124. How do you log information in your Python applications, and which logging library do you use?
Answer:
I use Pythons built-in logging library to write logs in my applications. I configure loggers with different levels such as DEBUG, INFO, WARNING, ERROR and CRITICAL. For example, I log important events like user logins or task completions at INFO level, and unexpected errors at ERROR level with stack traces. Logs can be written to the console in development and to files or external logging systems in production. I avoid using print statements in production code and rely on structured logging instead, which makes it easier to filter and search logs when debugging.

Q125. How do you structure configuration (development, staging, production) in your Python projects?
Answer:
I structure configuration by separating code from environment-specific settings. Typically, I use environment variables to store values like database URLs, secret keys and API endpoints. In Django or FastAPI, I have a settings module that reads from environment variables and provides defaults for development. For staging and production, I set different environment variables on the servers or in Docker Compose files. Sometimes I use separate settings files or profiles, but the main idea is that the same codebase can run in multiple environments just by changing configuration values, without editing code.

Q126. How do you handle sensitive information such as API keys and database passwords in your code?
Answer:
I never hardcode sensitive information like API keys or database passwords in the source code or commit them to version control. Instead, I store them in environment variables, secret managers or configuration files that are not checked into git. The application reads these values at runtime. Access to these secrets is limited to trusted team members and production systems. In Docker and CI/CD, I use secret management features to inject values securely. This reduces the risk of leaks and makes it easier to rotate keys if needed.

Q127. What is your approach to writing unit tests in Python?
Answer:
My approach is to focus on testing important business logic and edge cases. I try to write small tests that check one thing at a time and are easy to understand. For pure functions, I test different inputs and expected outputs. For APIs, I test both successful responses and common error situations, such as invalid data or missing permissions. I keep tests fast so they can run often, especially in CI. I also see tests as documentation: they show how code is supposed to behave, which helps me and other developers later.

Q128. How do you use pytest or Djangos test framework in your projects?
Answer:
In Django projects, I often use Djangos built-in test framework or combine it with pytest for more flexibility. I write test cases as classes or functions that set up test data, call views or APIs and assert on responses and database state. Pytest makes it easy to use fixtures for shared setup, like creating users or sample records. In FastAPI projects, I use pytest with FastAPIs TestClient to send HTTP requests to the app and check results. Tests are run locally and in CI pipelines to catch regressions before deployment.

Q129. How do you mock external services (like APIs or S3) in your tests?
Answer:
To mock external services, I replace real calls with fake ones during tests. In Python, I use unittest.mock or pytest-mock to patch functions or methods that call external APIs or S3. The mock returns predefined responses so tests do not depend on the real services. For example, when testing S3 uploads, I mock the boto3 clients upload method to pretend it succeeded. This makes tests faster, more reliable and safe, because they do not create real network traffic or modify real data.

Q130. How do you measure and improve the performance of a slow Python function or endpoint?
Answer:
To measure performance, I start by timing the function or endpoint using simple timing tools, logging or profilers. I also examine logs and monitoring dashboards to see where time is spent, such as in database queries or external calls. Once I know the bottleneck, I optimize targeted parts: for example, reduce database queries with better ORM usage, add indexes, cache results or move heavy work to background tasks. After each change, I measure again to confirm improvement. I try not to guess, but to use data to guide performance work.

Q131. Can you explain the main components of a Django project (models, views, templates, URLs)?
Answer:
The main components of a Django project are models, views, templates and URLs. Models define the data structure and map to database tables. Views contain the logic that handles a request and prepares a response. Templates define how HTML pages look when you use Django for server-side rendering. URLs map incoming paths to specific views. In API-only projects, we still use models and views, but instead of HTML templates we return JSON responses, often using Django REST Framework. These components work together so that a request comes in through a URL, a view processes it, and data from models is returned.

Q132. How do Django models map to database tables?
Answer:
Each Django model class typically maps to a single database table. The class name becomes the table name (with some automatic changes), and each field on the model becomes a column. For example, a model with a CharField and a DateTimeField will create corresponding columns in the table. Djangos ORM handles creating and updating tables through migrations, so you rarely write raw SQL for schema changes. Relationships like ForeignKey and ManyToManyField create links between tables. This mapping lets you work in Python objects while Django translates operations into efficient SQL queries.

Q133. What is the difference between function-based views and class-based views in Django?
Answer:
Function-based views are simple Python functions that take a request and return a response. They are easy to understand and good for small, straightforward logic. Class-based views are classes that provide reusable behaviour for common patterns, such as displaying a list or a detail page. They support inheritance and mixins, which makes it easier to share behaviour across many views. In API work, class-based views and viewsets are often preferred because they integrate well with DRF, but for very small endpoints I still sometimes use function-based views for simplicity.

Q134. How do you use Django REST Framework to build APIs?
Answer:
To build APIs with DRF, I define serializers to describe how model instances are converted to and from JSON, and views or viewsets to handle HTTP methods. A typical pattern is to create a ModelSerializer for a model and then a ModelViewSet that supports list, retrieve, create, update and delete operations. Routers map these viewsets to URLs automatically. I also configure authentication and permissions so only authorized users can access certain endpoints. DRF provides many helpful tools like pagination, filtering and browsable API, which speed up development of clean, consistent REST APIs.

Q135. What are serializers in DRF and how do you use them?
Answer:
Serializers in DRF are responsible for converting complex data, like Django model instances, into Python primitives that can be rendered as JSON, and for validating and deserializing incoming data. I usually use ModelSerializer to quickly create serializers based on models, specifying which fields are exposed. For create and update operations, serializers validate the input and either raise errors or save objects. I can also write custom validation methods for specific fields or cross-field checks. Serializers keep input/output logic separate from views, making code cleaner and easier to test.

Q136. How do you handle authentication and permissions in DRF?
Answer:
In DRF, authentication determines who the user is, and permissions determine what they are allowed to do. I configure authentication classes, such as token auth, session auth or JWT, in the settings or per-view. For permissions, I use DRFs built-in classes like IsAuthenticated or IsAdminUser, and write custom permission classes when I need role-based or object-level checks. For example, only the owner of a resource or an admin may edit it. These permission classes are applied on viewsets or views, so every request is checked automatically before the main logic runs.

Q137. How do you implement pagination in a DRF API?
Answer:
DRF supports pagination out of the box. I usually set a default pagination class in settings, such as PageNumberPagination or LimitOffsetPagination, and define a page size. When a list endpoint returns results, DRF automatically includes only one page of items and adds metadata like next and previous links and total count. Clients can request different pages using query parameters like page or limit and offset. For special cases, I can define custom pagination classes to change the format or behaviour. Pagination keeps responses small and improves performance for large datasets.

Q138. How do you implement filtering and searching in DRF endpoints?
Answer:
To add filtering and searching, I use DRFs filter_backends. Common ones are DjangoFilterBackend for field-based filtering, SearchFilter for simple search and OrderingFilter for sorting. In a viewset, I specify filter_backends and define filterset_fields, search_fields and ordering_fields. For example, in a shipments API, I might allow filtering by status and date range, searching by tracking number and ordering by created_at. For more complex needs, I sometimes write custom filters or use a dedicated filterset class. This approach keeps filtering logic clean and consistent across endpoints.

Q139. What are viewsets and routers in DRF, and when do you use them?
Answer:
Viewsets in DRF are classes that combine the logic for multiple HTTP methods (list, create, retrieve, update, delete) into one place. Instead of writing separate views for each action, I define a single viewset for a resource like ShipmentViewSet. Routers automatically generate the URL patterns for these viewsets, reducing boilerplate. I use viewsets and routers whenever I am building standard CRUD APIs, because they keep code short and organized. If I have a very custom endpoint that does not fit CRUD, I might use APIView or function-based views instead.

Q140. How do you organize a large Django/DRF project with many apps and modules?
Answer:
In a large Django/DRF project, I split features into separate apps, each responsible for one domain area like users, billing or logistics. Within each app, I group code into models, serializers, views, urls, services and tests. Shared utilities and base classes live in common or core apps. I keep settings and configuration organized, sometimes with separate files for different environments. Clear naming and consistent patterns are very important so developers can guess where things belong. I also rely on good documentation and code reviews to keep the structure healthy as the project grows.
Q141. How do you write custom middleware in Django, and when would you use it?
Answer:
Custom middleware in Django is a class with methods that process the request or response, such as __call__ or process_view, depending on the version. You add it to the MIDDLEWARE list in settings. I would use custom middleware when I need logic that should run for many requests, such as logging, tracking, security headers or simple access control. For example, I might create middleware that logs user IDs and paths they access, or that checks for a maintenance mode flag and returns a special response. Middleware is not for business logic but for cross-cutting concerns.

Q142. How do you handle file uploads and media storage in Django?
Answer:
In Django, I handle file uploads using FileField or ImageField on models and forms or DRF serializers that accept files. When a file is uploaded, Django stores it in the MEDIA_ROOT directory with a path controlled by upload_to. In production, I usually configure a remote storage backend, such as AWS S3, using libraries like django-storages. This way, files are not stored on the application server itself. I also define MEDIA_URL to serve or link to uploaded files. Permissions and access control are handled at the view or storage level, depending on the project needs.

Q143. How do you manage static files in a Django production deployment?
Answer:
For static files like CSS and JavaScript, Django uses the STATICFILES system. In production, I run collectstatic to gather files from all apps into a single directory. These static files are then served by a web server like Nginx, a CDN or a storage service like S3 with proper caching. The Django app itself usually does not serve static files in production for performance reasons. I configure STATIC_URL and STATIC_ROOT in settings and set up the web server to point to the collected files. This approach keeps page loads fast and reduces load on the application.

Q144. How do you handle transactions in Django when multiple database operations must succeed or fail together?
Answer:
In Django, I use transaction.atomic to group multiple database operations into a single transaction. Inside a with transaction.atomic(): block, if any operation raises an exception, all changes are rolled back, keeping the database consistent. I use this pattern when creating related objects that must either all exist or not exist at all, such as creating an order with related items and logs. I keep the transaction block as small as possible to avoid locking too much data. Django also lets me control isolation level and savepoints when I need finer control.

Q145. How do you use Djangos ORM for complex queries (joins, annotations, aggregations)?
Answer:
Djangos ORM supports complex queries through related lookups, annotations and aggregations. I use select_related and prefetch_related to follow foreign keys efficiently and avoid N+1 queries. For calculations, I use annotate and aggregate with functions like Count, Sum or Avg to compute values per group. I can chain filters across relationships, such as filtering shipments by related user fields. When queries become too complex, I sometimes use raw SQL or the extra() method, but I try to stay within the ORM as much as possible for portability and readability.

Q146. How do you debug and profile slow Django views or queries?
Answer:
To debug slow Django views, I first add logging around key parts to see which section is slow. I also use tools like Django Debug Toolbar in development to inspect SQL queries, their count and execution time. If I see many similar queries, I optimize with select_related or prefetch_related. For deeper profiling, I might use Python profilers like cProfile or line_profiler to find hotspots in code. Monitoring tools in production, such as APM agents, also help identify slow endpoints over time. Once I know the cause, I optimize the specific queries or code paths.

Q147. What security features in Django do you rely on (such as CSRF and XSS protection)?
Answer:
Django offers many built-in security features that I rely on. CSRF protection helps prevent cross-site request forgery in forms and authenticated POST requests. The templating system automatically escapes variables to reduce XSS risks when rendering HTML. Django also has protections against SQL injection by using parameterized queries in the ORM. Features like secure session cookies, password hashing and built-in authentication models are important too. I make sure SECURITY-related settings, such as SECURE_HSTS_SECONDS and SECURE_SSL_REDIRECT, are configured properly in production to enforce HTTPS.

Q148. How do you enforce coding standards and code reviews in your Django projects?
Answer:
To enforce coding standards, I use tools like flake8, black or isort to check style and formatting. These tools can run locally and in CI, so code that does not meet standards is flagged early. For code reviews, we use pull requests where another developer reviews changes, checks for readability, correctness and security issues, and suggests improvements. I try to keep pull requests small and focused, which makes reviews easier. This process helps keep the Django codebase consistent and reduces bugs.

Q149. What are some common mistakes you have seen or made when working with Django?
Answer:
Common mistakes include writing heavy logic directly in views instead of using services, forgetting to use select_related or prefetch_related and causing N+1 query issues, and not handling migrations carefully, which can break production. I have also seen people hardcode settings that should be in environment variables, or expose too much data in serializers without proper permissions. Early in my experience, I sometimes mixed presentation and business logic, which made tests harder. Over time, I learned to structure code better and pay more attention to performance and security.

Q150. How would you migrate a legacy Django app to a more modern structure or newer Django version?
Answer:
To migrate a legacy Django app, I would first set up good tests to catch regressions. Then I would upgrade Django and third-party packages step by step, following the official release notes and deprecation warnings. For structure, I would gradually move logic from large views or models into smaller apps and service layers, without changing behaviour. I would also modernize configurations, such as switching to newer middleware style or URL patterns. Doing this in small, reviewed changes reduces risk and makes it easier to roll back if necessary.

Q151. What are the advantages of FastAPI compared to Django REST Framework?
Answer:
FastAPI has several advantages, especially for pure API services. It is very fast and built on modern async stack, which helps with high-concurrency workloads. It uses Python type hints heavily, so request and response models are clear and validated automatically. FastAPI also generates interactive API docs with Swagger and ReDoc out of the box, which saves time. Compared to DRF, FastAPI can feel lighter and more direct for greenfield API projects, while DRF is great when you are already in the Django ecosystem or need many batteries included.

Q152. How do you define routes and request/response models in FastAPI?
Answer:
In FastAPI, I define routes using decorators on functions, like @app.get("/items") or @router.post("/tenders"). For request and response models, I create Pydantic classes with the expected fields and types. I then use them as type hints in function parameters and return types. For example, def create_tender(tender: TenderCreate) -> TenderOut. FastAPI reads these hints and automatically parses incoming JSON into model instances, validates them and serializes responses. Routers allow grouping related endpoints and including them in the main app with prefixes and tags.

Q153. How do you use Pydantic models for validation in FastAPI?
Answer:
Pydantic models are central to validation in FastAPI. I define models with fields and types, and can add constraints like max_length or regex. When a request body is expected, I declare a parameter of that model type in the route function. FastAPI automatically parses the JSON body into a Pydantic instance and validates all fields. If data is missing or invalid, FastAPI returns a clear 422 error response with details. I can also use Pydantic for response models, ensuring consistent output shapes. Custom validators let me add more complex rules when needed.

Q154. How do you handle dependency injection in FastAPI (for database sessions, services, etc.)?
Answer:
FastAPI has a built-in dependency injection system using the Depends helper. I define small functions that create or provide resources, such as a database session or the current user, and then use them as parameters with Depends in route functions. For example, db: Session = Depends(get_db). FastAPI calls these dependency functions automatically and passes the result into the endpoint. This keeps code clean and makes it easy to reuse common logic like auth checks, service instances or configuration access across many routes.

Q155. How do you implement authentication and authorization in FastAPI?
Answer:
Authentication and authorization in FastAPI are usually done with dependencies. I create a dependency that reads a JWT or token from headers, verifies it and returns the current user object. If verification fails, it raises an HTTPException with 401. For authorization, I can create another dependency that checks the users role or permissions before allowing certain actions. These dependencies are added to routes or routers, so they run before the main logic. FastAPI also provides security utilities that help with common schemes like OAuth2 with password flow or API keys.

Q156. How do you handle background tasks in FastAPI?
Answer:
FastAPI provides a BackgroundTasks helper for simple background work. In a route function, I accept a BackgroundTasks parameter and then add tasks to it, such as sending an email after returning a response. FastAPI runs these tasks after the response is sent, without blocking the client. For heavier or scheduled tasks, I integrate FastAPI with Celery, using Celery workers for long-running jobs while the FastAPI app handles HTTP requests. This combination gives both simple and advanced background processing options.

Q157. How do you configure CORS in a FastAPI application?
Answer:
To configure CORS, I use FastAPIs CORSMiddleware. In the app setup, I add the middleware with allowed origins, methods and headers, like allowing the frontend domain to call the API. For example, app.add_middleware(CORSMiddleware, allow_origins=["https://frontend.com"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]). This setup prevents browser CORS errors when the frontend and backend run on different domains or ports, while still keeping some control over who can call the API.

Q158. How do you document APIs automatically using FastAPIs OpenAPI/Swagger support?
Answer:
FastAPI automatically generates OpenAPI schema and Swagger UI from route definitions and type hints. When I define endpoints with proper models and docstrings, FastAPI builds documentation that can be viewed at /docs or /redoc. I can add extra metadata like tags, descriptions and examples in the route decorators. This makes it easy for frontend developers and testers to understand and try APIs without separate documentation work. Keeping the docs close to the code reduces the chance they become outdated.

Q159. How do you structure a medium-sized FastAPI project for maintainability?
Answer:
For a medium-sized FastAPI project, I separate the app into modules such as api, models, schemas, services and core. Under api, I have routers for different features (auth, users, tenders, etc.). Models contain database definitions, schemas contain Pydantic models and services hold business logic. A core or config module manages settings, security utilities and startup tasks. This structure keeps files focused and makes it easier to navigate the codebase. Tests are also organized by feature, matching the main modules.

Q160. How do you deploy a FastAPI app in production (for example using Uvicorn, Gunicorn, or containers)?
Answer:
To deploy FastAPI, I usually containerize the app with Docker and run it behind a production server like Gunicorn with Uvicorn workers. The container exposes a port, and an Nginx or cloud load balancer handles TLS and routes traffic. Environment variables configure the database and other services. I use CI/CD to build and push images and then pull and restart containers on the server. Health checks and logging are set up to monitor the app. This pattern works well on cloud platforms like AWS EC2 or container services.

Q161. What is Celery and why would you use it in a backend system?
Answer:
Celery is a distributed task queue that lets you run work in the background, outside the main web request. You use it when you have tasks that are slow or can be done later, such as sending emails, processing files or syncing data with external APIs. Instead of blocking the user while the task finishes, you push the job to Celery and return a response quickly. Workers then pick up tasks from a broker like Redis and execute them. This improves user experience and helps the system handle more load.

Q162. What kinds of tasks do you typically run in Celery (from your resume projects)?
Answer:
In my projects, I have used Celery for tasks such as syncing marketing data from external APIs (Thirdi-AI), sending notifications and alerts based on shipment status (DHL Logistics Platform), and running periodic cleanup or reporting jobs. These tasks often involve network calls, heavy calculations or operations that can be delayed a bit without harming the user experience. By moving them to Celery, the main API stays responsive, and we can schedule them to run at specific intervals or in response to certain events.

Q163. How do you configure Celery with Redis as a broker and result backend?
Answer:
To configure Celery with Redis, I set the broker URL and result backend URL to point to the Redis instance, for example redis://localhost:6379/0. In the Celery app configuration, I define these URLs and other options like timezone and task serialization. I run Redis as a separate service, often in Docker, and start Celery workers with a command like celery -A project worker -l info. This setup allows Celery to send tasks to Redis, where workers pick them up and store results or statuses back if needed. Configuration is usually shared across environments using settings files.

Q164. How do you schedule periodic tasks with Celery Beat?
Answer:
Celery Beat is a scheduler that sends tasks to the broker at regular intervals. I configure periodic tasks in the Celery settings, either using CELERY_BEAT_SCHEDULE or a similar structure, where I map task names to schedules like every minute, hourly or daily. Then I run a Celery Beat process along with the workers. Beat sends messages to the queue at the specified times, and workers execute the tasks. I use this for jobs like syncing external data, generating daily reports or cleaning old records.

Q165. How do you handle retries and error handling in Celery tasks?
Answer:
In Celery tasks, I handle errors using try/except blocks for expected failures and by configuring automatic retries for temporary problems. Celery tasks can be given retry parameters, such as max_retries and countdown, or can call self.retry() inside the task when an error occurs. I log exceptions with enough context to debug later. For permanent errors that should not be retried, I let the exception bubble up or handle it and mark the task as failed. This combination ensures that transient issues are retried but true bugs are not hidden.

Q166. How do you monitor Celery workers and task queues?
Answer:
To monitor Celery, I use tools like Flower or Celerys built-in monitoring commands. Flower provides a web UI where I can see active, scheduled and failed tasks, as well as worker status. Logs from Celery workers also show task execution and errors. In some environments, I integrate Celery metrics into a monitoring system or dashboards, tracking the number of tasks, failure rates and processing times. This helps me detect bottlenecks or problems, such as stuck workers or repeated failures, and take action.

Q167. What are some best practices you follow when writing Celery tasks?
Answer:
When writing Celery tasks, I keep them small and focused, so each task does one thing well. I make tasks idempotent when possible, meaning running them twice does not cause incorrect results. I avoid passing large objects; instead, I pass IDs and fetch data inside the task. I handle exceptions clearly and use retries for temporary issues. I also avoid heavy database work in tight loops and use bulk operations. Finally, I document what each task does and when it should be used, so the task queue remains understandable as the project grows.

Q168. What is Redis and how have you used it in your projects?
Answer:
Redis is an in-memory data store that can be used as a cache, message broker or simple database. In my projects, I have used Redis mainly as the broker and result backend for Celery and sometimes as a cache for frequently accessed data. Because Redis is very fast, it is good for storing short-lived data like session tokens, rate-limit counters or temporary results. Running Redis in Docker or as a managed service makes it easy to integrate into Python backends.

Q169. What data structures in Redis have you used (strings, lists, sets, hashes), and for what?
Answer:
The most common Redis structure I use is simple strings, which store values like tokens or flags. I have also used hashes to store grouped key-value pairs for a single object, such as user session data. Lists can be used as simple queues or logs, where new items are pushed to one end. Sets are useful when I need to store unique items and check membership quickly. These structures, combined with expiration times, solve many small caching and coordination tasks without a full database query.

Q170. How do you handle data expiration and cleanup in Redis?
Answer:
Redis supports key expiration, so I usually set a TTL (time-to-live) when storing short-lived data like sessions, verification codes or cache entries. This means keys disappear automatically after a given time, keeping the store clean. For data that does not have a fixed lifetime but still needs cleanup, I may run periodic Celery tasks that scan keys by pattern and delete old ones. Using expiration wisely prevents Redis from growing indefinitely and keeps memory usage under control.

Q171. How do you decide between PostgreSQL and MySQL for a new project?
Answer:
In many cases I prefer PostgreSQL because it has strong features, good JSON support and works very well with Django. I pick MySQL when the team or existing systems are already using it and we want to stay consistent. I also look at hosting options, required features and expected workload. For complex queries, reporting and strict data types, PostgreSQL often gives more flexibility. For simpler workloads where MySQL is already used, MySQL can be fine. The key is to understand the project needs instead of saying one database is always best.

Q172. What is database normalization, and how far do you typically normalize your schemas?
Answer:
Normalization is the process of structuring a database to reduce duplication and improve data integrity. It usually means splitting data into related tables and using foreign keys. I generally normalize up to third normal form: each table represents one concept, fields depend on the key and there is minimal repeated data. However, I am practical; if full normalization makes queries too complex or slow, I may denormalize some parts, for example storing summary fields. The goal is a balance between clean design and good performance.

Q173. How do you design tables for entities like users, roles, and permissions?
Answer:
For users, I create a table with basic profile fields, login information and status flags. Roles are stored in a separate table with role names and descriptions. A linking table connects users to roles (many-to-many), so each user can have multiple roles. Permissions can be modeled as another table listing actions, with relationships to roles. In some projects, I simplify and use a fixed set of roles with hardcoded permissions. In all cases, I keep the design flexible enough to add new roles or change mappings without breaking existing data.

Q174. How do you create and use indexes to improve query performance?
Answer:
Indexes are created on columns that are often used in WHERE clauses, JOINs or ORDER BY. In Django, many foreign keys automatically get indexes, but I also add indexes on fields like status, email or created_at when they are part of filters. I monitor slow queries using logs or database tools, and if I see full table scans on large tables, I consider new indexes. However, I try not to add too many, because each index has a cost on writes. The goal is to speed up common queries that matter for performance.

Q175. How do you analyze and optimize a slow SQL query?
Answer:
To analyze a slow query, I first inspect it, then use tools like EXPLAIN in PostgreSQL or MySQL to see the execution plan. This shows whether the database is using indexes or scanning whole tables. Based on the plan, I may add or adjust indexes, rewrite the query to be simpler, or reduce the amount of data scanned by adding better filters. In Django, this can mean using select_related, prefetch_related or values() to avoid unnecessary columns and joins. I measure before and after changes to confirm the improvement.

Q176. How do you handle database migrations in Django or FastAPI projects?
Answer:
In Django, I use the built-in migration system: after changing models, I run makemigrations and migrate. I review migration files to ensure they look correct, especially for destructive changes. In FastAPI projects with SQLAlchemy or similar, I use tools like Alembic to manage migrations, writing scripts that alter tables, add columns or change constraints. Migrations are run in staging first and then in production, usually as part of the deployment pipeline. I avoid making big risky changes in one step and prefer several smaller, safer migrations.

Q177. How do you ensure data integrity when multiple services or workers are updating the database?
Answer:
To ensure data integrity, I rely on database constraints (foreign keys, unique constraints, not-null) and transactions. Business rules that must always hold are enforced at the database level when possible, not only in application code. In applications, I use transactions to group related updates so they either all succeed or all fail. I also design APIs carefully to avoid race conditions, for example by using row-level locks or optimistic locking patterns when needed. Clear rules about who can update what and when also help reduce conflicts.

Q178. How do you handle transactions and rollbacks in your applications?
Answer:
I handle transactions using framework tools. In Django, I wrap related operations inside transaction.atomic() blocks. If any operation inside fails, an exception is raised and all changes are rolled back. In other environments, I use the database sessions transaction controls similarly. I keep transactions as small as possible and avoid long-running operations inside them, to reduce locking. Proper error handling is important: I log failures with enough detail to understand what happened and make sure users receive clear feedback.

Q179. How do you design audit logs or change history in your database?
Answer:
For audit logs, I create tables that record key changes: who made the change, when, what record was affected and what fields changed. Sometimes I store old and new values as JSON for flexibility. In Django, I can use signals or model methods to write to audit tables whenever certain actions occur. In more advanced setups, I might use a separate event table or event-sourcing style, but even simple logs can be very helpful for debugging and compliance. The main aim is to be able to answer who changed what and when.

Q180. How do you back up and restore databases in your projects?
Answer:
For backups, I rely on database tools or cloud provider features. In AWS RDS, automated snapshots and point-in-time recovery are common. For self-managed databases, I use tools like pg_dump or mysqldump and store backups securely, often with encryption. Restoring is tested in staging: I take a backup, restore it to a test database and verify that the application works. It is important to have a clear backup schedule and retention policy, so we can recover from data loss or corruption quickly.

Q181. How do you write a basic Dockerfile for a Python backend service?
Answer:
A basic Dockerfile for a Python backend starts from a lightweight base image like python:3.x-slim. Then I set a working directory, copy the requirements file, install dependencies with pip, and copy the application code. I set environment variables such as PYTHONDONTWRITEBYTECODE and PYTHONUNBUFFERED, and define the command to run the server, for example using Gunicorn or Uvicorn. I also create a non-root user and switch to it for better security. Finally, I expose the application port. This structure makes it easier to cache layers and build images quickly.

Q182. What are best practices you follow in Dockerfiles (layers, .dockerignore, non-root user)?
Answer:
Best practices include minimizing the image size by using slim base images and installing only needed packages. I use a .dockerignore file to exclude files like .git, tests and local data, so they are not copied into the image. I order Dockerfile instructions to take advantage of layer caching, putting rarely changing steps later. I also create a non-root user and switch to it before running the app, reducing security risks. Cleaning up temporary files and using multi-stage builds for production images can further improve size and security.

Q183. How do you use Docker Compose in local development?
Answer:
In local development, I use Docker Compose to define and run multiple services together, such as the backend app, database and Redis. The docker-compose.yml file specifies images, environment variables, volumes and ports. With a single command, docker-compose up, I can start the full stack that matches production closely. This makes onboarding new developers easier and reduces it works on my machine problems. I often mount the source code as a volume so changes are reflected without rebuilding the image during active development.

Q184. How do you debug issues inside a running Docker container?
Answer:
To debug inside a container, I use docker exec -it <container> /bin/bash (or sh) to get a shell. From there, I can inspect logs, configuration files and running processes. I may run Python shells or management commands to test behaviour. I also check environment variables to ensure they are set correctly. For networking issues, I verify connectivity to other services from inside the container. Logging to stdout and using structured logs also helps, because Docker captures those logs and they can be viewed easily with docker logs.

Q185. How have you used AWS EC2 to host your backend services?
Answer:
I have used AWS EC2 to run Dockerized backend applications. Typically, I create an EC2 instance, install Docker and Docker Compose, clone the project or pull images from a registry, and run the services with Compose. Security groups are configured to allow traffic on HTTP/HTTPS ports and any internal ports needed. I also set up environment variables or configuration files on the instance. In some setups, an Application Load Balancer sits in front of EC2 instances to distribute traffic and handle SSL termination.

Q186. How have you configured AWS S3 for storing and serving files?
Answer:
For S3, I create a bucket, configure proper region and set policies so only authorized users or services can access it. In applications, I use AWS SDK (boto3) with credentials passed via IAM roles or environment variables. Uploaded files are stored with structured keys, often including IDs and folder-like paths. For public assets, I may enable static website hosting or use CloudFront as a CDN. For private documents, I generate pre-signed URLs that allow temporary access without exposing the bucket publicly.

Q187. What is AWS RDS and how did you use it in the ProcureMate project?
Answer:
AWS RDS is a managed relational database service that supports engines like PostgreSQL and MySQL. In ProcureMate, we used RDS to host the main application database. AWS handled backups, patching and basic monitoring, while we focused on schema design and queries. The FastAPI app connected to RDS using a connection string, and security groups restricted access to only the application instances. Using RDS reduced operational work compared to managing our own database servers.

Q188. How do you secure access to S3 buckets and RDS instances?
Answer:
To secure S3, I use IAM roles and policies to grant least-privilege access, disabling public access unless absolutely needed. For RDS, I place the instance in a private subnet, restrict security groups to only allow traffic from the application servers and require strong passwords or IAM authentication. I also enforce encrypted connections (TLS) where possible. Credentials are stored in environment variables or secret managers, not in code. Regular reviews of permissions and logs help catch misconfigurations.

Q189. What steps do you include in a typical CI/CD pipeline for a Python backend service?
Answer:
A typical CI/CD pipeline includes steps to checkout code, set up Python, install dependencies, run unit tests and linters, and then build a Docker image. If tests pass, the image is pushed to a registry and a deployment step is triggered, such as updating containers on a server or calling a deployment script. Environment-specific configuration is handled via variables in the CI system. Notifications are sent if builds or deployments fail. This pipeline ensures that code is tested and packaged consistently before reaching production.

Q190. How do you roll back a deployment if something goes wrong?
Answer:
To roll back, I keep previous versions of the Docker image or application package. If a new deployment causes problems, I redeploy the last known good version, either by changing the image tag in Docker Compose or by using deployment tools that support rollbacks. Database migrations need extra care; I avoid destructive changes and sometimes write down-migrations. Good monitoring and logs help me detect issues quickly, so I can roll back before many users are affected.

Q191. How do you manage your time when working on multiple projects like Thirdi-AI, DHL, and ProcureMate?
Answer:
When working on multiple projects, I rely on clear priorities and planning. I start by understanding which tasks are most important or time-sensitive and make a daily list. I break work into small, clear steps so I can switch contexts more easily. I also block focused time for deep work and avoid jumping between tasks too often. Regular check-ins with my lead or team help me confirm that I am focusing on the right things. If I see that my workload is becoming unrealistic, I communicate early instead of waiting until deadlines are at risk.

Q192. How do you communicate progress and blockers to your team or manager?
Answer:
I communicate progress through daily or regular updates, such as standup meetings or messages in our team chat. I briefly share what I did yesterday, what I plan to do today and any blockers I have. When I am stuck, I explain the problem clearly, what I have tried and where I need help. For larger tasks, I sometimes share partial results or small demos so the team can see progress early. I try to be honest and realistic, not overpromise. Good communication builds trust and helps the team adjust plans if needed.

Q193. Can you describe a situation where you had to explain a technical concept to a non-technical stakeholder?
Answer:
In projects like Thirdi-AI or ProcureMate, I often had to explain technical topics like rate limits or data sync delays to product or business people. For example, when they asked why dashboard data might be a few minutes old, I explained that we fetch data from external APIs in the background to keep the app fast. I used simple language, saying, We call their system every few minutes, so there can be a small delay, but this keeps everything stable. I avoid jargon, use analogies when helpful and check that they understood by asking if it makes sense.

Q194. How do you handle situations when requirements change late in the development cycle?
Answer:
When requirements change late, I first try to understand the reason and what exactly must change. Then I look at the impact on the existing code, tests and timeline. I explain to the team or stakeholder what trade-offs are involved: for example, which parts will take more time or which other tasks may be delayed. If possible, I suggest a phased approach: implement the most critical part now and leave nice-to-have changes for a later iteration. I stay calm and flexible, but I also try to protect code quality by not rushing in a way that will cause many future bugs.

Q195. How do you give and receive feedback in code reviews?
Answer:
In code reviews, I focus on the code, not the person. When giving feedback, I try to be specific and kind, explaining why I suggest a change, such as readability, performance or security. I also point out things that are done well. When I receive feedback, I do not take it personally; I see it as a chance to learn. I ask questions if something is not clear and explain my reasoning if I made a choice intentionally. This open attitude makes reviews a positive part of the development process.

Q196. How do you keep yourself updated with new tools and frameworks in the Python ecosystem?
Answer:
I stay updated by following official documentation, blogs and some developer communities. I read release notes for the main tools I use, like Django, DRF, FastAPI and Celery. I sometimes watch conference talks or tutorials online to learn new patterns. When I see a new tool that looks useful, I try it in a small side project or proof of concept before suggesting it at work. I do not chase every new thing, but I try to slowly improve my toolbox over time.

Q197. How do you handle conflicts in a team when opinions differ on a technical solution?
Answer:
When there is a technical disagreement, I first listen carefully to understand all viewpoints. I try to focus the discussion on requirements and trade-offs, not on who is right. We compare options in terms of complexity, performance, maintainability and future changes. Sometimes we do a quick proof of concept or look for best practices in documentation. If we still cannot agree, I respect the final decision of the tech lead or team norm. The most important thing is to move forward together and not let conflicts become personal.

Q198. How do you prioritize bugs versus new feature work when both are urgent?
Answer:
I look at the impact and risk of each item. Critical bugs that affect many users or break key flows usually come first, because they can damage trust and block other work. New features that are tied to business deadlines also get high priority. I discuss priorities with the product owner or manager, sharing my view on technical risks. Sometimes we can do a quick hotfix for a bug and then continue features, or we may delay a feature slightly to stabilize the system. Clear communication helps make these trade-offs transparent.

Q199. If we call your current manager, what do you think they will say about you?
Answer:
I believe my current manager would say that I am reliable, willing to learn and careful with my work. They would probably mention that I take ownership of tasks, communicate honestly about progress and do not hide problems. They might also say that I am calm under pressure and open to feedback. I still have many things to improve, but I try to show steady growth in my skills and contributions. Overall, I hope they would describe me as a solid team member they can trust with backend responsibilities.

Q200. Why should we hire you for this backend developer role based on your resume?
Answer:
You should hire me because I already work with the same kind of stack and problems that your role describes. I have hands-on experience with Python, Django, DRF, FastAPI, Celery, Redis, PostgreSQL/MySQL, Docker and AWS from real projects like Thirdi-AI, DHL Logistics Platform and ProcureMate. I understand how to design and build RESTful APIs, background tasks and cloud deployments, not just in theory but in practice. At the same time, I am still early in my career and very motivated to learn. I bring a mix of practical experience, clean coding habits and a positive, team-focused attitude.


